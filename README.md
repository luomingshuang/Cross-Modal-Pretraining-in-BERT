# Cross-Modal Pretraining in BERT[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

I just want to have a collection of works (including papers and codes) about the cross-modal pretraining in BERT. So I will try my best to record the papers and codes in these years. If you have any other news can enrich and improve this page, I am very happy to receive your messages. Hope my work can help you!

#### Why cross-modal pretraining in BERT?
BERT has been a very popular model architecture in AI community. 
* Strong long sequence modeling capabilities
* Self attention mechanism

Cross-modal learning mechanism is more similar to the learning method of the human brain.

## Contributing
<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

Please help contribute this list by contacting [me](https://jason718.github.io/) or add [pull request](https://github.com/jason718/Awesome-Self-Supervised-Learning/pulls)

Markdown format:
```markdown
- Paper Name. 
  [[pdf]](link) 
  [[code]](link)
  - Author 1, Author 2, and Author 3. *Conference Year*
```

## Table of Contents
* VideoBERT: A Joint Model for Video and Language Representation Learning.[[pdf]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf). *2019 ICCV.*  
* Fusion of Detected Objects in Text for Visual Question Answering.[[pdf]](https://arxiv.org/abs/1908.05054)
* VisualBERT: A Simple and Performant Baseline for Vision and Language.[[pdf]](https://arxiv.org/abs/1908.03557).[[code]](https://github.com/uclanlp/visualbert)
