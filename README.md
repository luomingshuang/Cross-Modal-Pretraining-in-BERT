# Cross-Modal Pretraining in BERT[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

I just want to have a collection of works (including papers and codes) about the cross-modal pretraining in BERT. So I will try my best to record the papers and codes in these years. If you have any other news which can enrich and improve this page, I am very happy to receive your messages. Hope my work can help you!

#### Why cross-modal pretraining in BERT?
BERT has been a very popular model architecture in AI community. 
* Strong long sequence modeling capabilities
* Self attention mechanism

Cross-modal learning mechanism is more similar to the learning method of the human brain.

## Contributing
<p align="center">
  <img src="http://cdn1.sportngin.com/attachments/news_article/7269/5172/needyou_small.jpg" alt="We Need You!">
</p>

Please help contribute this list by contacting [me](739314837@qq.com) or add [pull request](https://github.com/luomingshuang/Cross-Modal-Pretraining-in-BERT/pulls)

Markdown format:
```markdown
- Paper Name. 
  [[pdf]](link) 
  [[code]](link)
  - Author 1, Author 2, and Author 3. *Conference Year*
```

## Table of Contents
#### 2020

- 【ICLR】VL-BERT: Pre-training of Generic Visual-Linguistic Representations.[[pdf]](https://arxiv.org/abs/1908.08530).[[code]](https://github.com/jackroos/VL-BERT).   
  *Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai*
- 【Arxiv】ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data.[[pdf]](https://arxiv.org/abs/2001.07966).   
  *Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti*

#### 2019

- 【ICCV】VideoBERT: A Joint Model for Video and Language Representation Learning.[[pdf]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf). 
  *Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid*
- 【Arxiv】Fusion of Detected Objects in Text for Visual Question Answering.[[pdf]](https://arxiv.org/abs/1908.05054). 
  *Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter*
- 【Arxiv】VisualBERT: A Simple and Performant Baseline for Vision and Language.[[pdf]](https://arxiv.org/abs/1908.03557).[[code]](https://github.com/uclanlp/visualbert).
  *Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang*
- 【AAAI】Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.[[pdf]](https://arxiv.org/abs/1908.06066).
  *Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou*
- 【Arxiv】ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.[[pdf]](https://arxiv.org/abs/1908.02265).[[code]](https://github.com/facebookresearch/vilbert-multi-task).
  *Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee*
- 【EMNLP】LXMERT: Learning Cross-Modality Encoder Representations from Transformers.[[pdf]](https://arxiv.org/abs/1908.07490).[[code]](https://github.com/airsplay/lxmert).
  *Hao Tan, Mohit Bansal* 
